{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May 11 07:58:09 2023\n",
    "\n",
    "@author: BigD\n",
    "\"\"\"\n",
    "#Import all moduels as required\n",
    "import astropy as ap\n",
    "from astropy.stats import poisson_conf_interval\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "from scipy.stats import (moyal, invgamma, lognorm, burr, beta, gamma, expon, exponpow, chi2, cauchy) \n",
    "from scipy import stats\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "\n",
    "\n",
    "# STEP 1 ORGANISE THE DATA\n",
    "\n",
    "# Import the crossmatched data \n",
    "data = Table.read('/Users/BigD/cloudstor/Data/EMU - GAMA - G23')\n",
    "data = data.to_pandas()\n",
    "\n",
    "#G23_data = Table.read('C:/Users/BigD/cloudstor/Data/GAMA/GAMA - Michael Brown/GAMA_G23_v01_mu3_20191202.fits')\n",
    "#G23_data = G23_data.to_pandas()\n",
    "\n",
    "#import the EMU data\n",
    "EMU_data = Table.read('/Users/BigD/cloudstor/Data/EMU/G23-ASKAP-EMUES-master-cat.fits', format='fits')\n",
    "EMU_data= EMU_data.to_pandas()\n",
    "\n",
    "#merge these two groups so that you have a ~40,000 sources table with 93 columns.\n",
    "data = pd.merge(data, EMU_data, how=\"outer\", on = 'Source_Name')\n",
    "data = data[['Source_Name','RA_y','E_RA_y','DEC_y','E_DEC_y' ,'Total_flux_y','E_Total_flux_y','Z','NQ','Separation_x']]\n",
    "data = data.rename(columns = {\"RA_y\":\"RA\", 'E_RA_y':'E_RA', 'DEC_y':'DEC', 'E_DEC_y':'E_DEC','Total_flux_y':'Total_flux','E_Total_flux_y':'E_Total_flux','Separation_x':'Separation' })\n",
    "\n",
    "#del(EMU_data, data)\n",
    "\n",
    "#We now have the crossmatched data set of EMU and G23, outlining:\n",
    "print(list(data))\n",
    "\n",
    "#create a column containing the rel error\n",
    "data['rel_err']=data['E_Total_flux']/data['Total_flux']\n",
    "\n",
    "\n",
    "#%% STEP 2 Visualise the data\n",
    "\n",
    "from observationsofdataset import Visualiseradiodata\n",
    "\n",
    "Visualiseradiodata(data)\n",
    "\n",
    "#we can see that there needs to be a lot of completness corrections\n",
    "\n",
    "#%% STEP 3 CREATE THE PDFS FOR MISSING REDSHIFT AND REDSHIFT ERRORS\n",
    "#NOTE THIS STEP WILL CREATE A DATAFRAME DF THAT IS POSITIVE FLUX AND Z>0\n",
    "\n",
    "#3.1 create pdfs for the redshift\n",
    "from completenesscorrection import getpdfs\n",
    "tol = 10 #the minimum number of data points in each bin to form a probability distribution function\n",
    "n = 10 #the number of magnitude bins the data will be seperated into\n",
    "df, pdfs, labels, use_labels, bins = getpdfs(data,n,tol)\n",
    "\n",
    "\n",
    "#3.2 Show what portion of the data set was used to create pdf's\n",
    "fig1, ax=plt.subplots(1,1)\n",
    "for label in labels:\n",
    "    #print(len(df[df['bin']==label]['mag']))\n",
    "    ax.hist(df[df['bin']==label]['mag'], alpha = 1, rwidth=1, range= (min(df['mag']),max(df['mag'])), bins=200,label=label)\n",
    "ax.set_xlabel('$Mag_{AB}$')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.suptitle('The different magnitudes that created the pdfs')\n",
    "plt.legend(fontsize = 10)\n",
    "\n",
    "\n",
    "v = np.sum([df['bin'].str.contains(label).sum() for label in use_labels])#the number of data points each mag bin collected\n",
    "v = \"{0:.3%}\".format(v/len(df))\n",
    "print(f\"pdf's were calculated from {v} of the datapoints that have a positive flux and greater than zero redshift\")\n",
    "\n",
    "\n",
    "#%% STEP 4 FILL IN THE MISSING DATA USING THE CREATED PDF'S\n",
    "\n",
    "#Calculate the magnitudes for each data point\n",
    "#Convert everything to AB magnitude\n",
    "mag = np.add(25, np.multiply(-2.5, np.log10(data['Total_flux'])))\n",
    "#add the column of magnitude to the dataset\n",
    "data['mag'] = list(mag)\n",
    "\n",
    "#Use the bins that the pdf's were created on, however add an upper and lower bound [ 0,50]\n",
    "# so to capture anything that falls outside the range covered in the spectro sample\n",
    "bins.append(50) #upper boundary\n",
    "bins.insert(0,0) #lower boundary\n",
    "#We must also create an upper bound for our labels\n",
    "labels.append(str(labels[-1].replace('<',\">\")))\n",
    "\n",
    "\n",
    "#Bin in accordance with the labels the pdf's were calculated for\n",
    "data['bin'] = pd.cut(np.array(data['mag']), bins= bins, labels = labels[0:])\n",
    "#Check the distribution\n",
    "print(data['bin'].value_counts(dropna=False))  #Check the distribution of the bins \n",
    "\n",
    "#Count what percentage of data the pdf's don't account for\n",
    "v = sum([data['bin'].value_counts(dropna=False)[label] for label in use_labels])\n",
    "v = \"{0:.3%}\".format(v/len(data))\n",
    "print(f\"The pdf's can be utilised for {v} of the EMU data\")\n",
    "\n",
    "\n",
    "# 4.1 Fill in the missing data and draw comparison of the mock and given data\n",
    "from fillinmissing import Fillinmissing\n",
    "df= Fillinmissing(data, bins, labels, use_labels, pdfs)\n",
    "\n",
    "\n",
    "#THIS IS WHERE I'M LOSING MY BIG FLUX READINGS< LOSING ABOUT 590 readings with only a 98.475\n",
    "\n",
    "#For now this is good enough lets continue on, with the 98.475% data for the luminosity functions.\n",
    "df = df[df['Z']>0]\n",
    "\n",
    "#Display a magnitude vs redshift graph for the filled in data\n",
    "fig2, ax =plt.subplots(1,1)\n",
    "ax.scatter(df['Z'],np.log10(df['Total_flux']))\n",
    "ax.set_xlabel('Redshift')\n",
    "ax.set_ylabel('Flux (Jy)')\n",
    "\n",
    "\n",
    "#%% For now this is good enough lets continue on, with the 98.475% data for the luminosity functions. #99.892%\n",
    "#reset the index of the dataframe\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#%% STEP 5 Calculate the luminosity's for everything\n",
    "\n",
    "#5.1 Find the turnaround of the flux\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "counts, bin_edges = np.histogram(np.log10(df['Total_flux']), bins=200)\n",
    "ax1.hist(np.log10(df['Total_flux']), bins = 200)\n",
    "ax1.set_title('The total flux as measured by EMU')\n",
    "ax1.set_xlabel('$Log_{10} S (Jansky)$')\n",
    "ax1.set_ylabel('counts') \n",
    "\n",
    "idx = np.where(counts==max(counts))[0][0]\n",
    "flux_limit=(10**bin_edges[idx])\n",
    "print('the flux limit is ', (\"{:.5f}\".format(10**bin_edges[idx])))\n",
    "\n",
    "#5.2 To find the flux limit lets reduce the data set to those values with only a positive redshift\n",
    "df=df[df['Z']>0]\n",
    "#reset the index\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "fig2, ax1 = plt.subplots()\n",
    "ax1.scatter(df['Z'],np.log10(df['Total_flux']))\n",
    "ax1.set_title('The distribution of flux readings across\\n the EMU GAMA Survey')\n",
    "ax1.set_xlabel('Redshift')\n",
    "ax1.set_ylabel('$Log_{10} S (Janksy)$')\n",
    "\n",
    "#5.4 Let's calculate the luminosity for everything\n",
    "#According to GÃ¼rkan the flux limits is 0.2mJy or 200uJy\n",
    "flux_limit = 200e-6\n",
    "\n",
    "#set parameters\n",
    "radio_index=-0.7\n",
    "#What is the survey radio frequency\n",
    "EMU_Freq = 0.8875#Ghz #997 Mhz\n",
    "#What is the frequency we wish to compare with (literature is normally 1.4Ghz)\n",
    "Comp_Freq = 1.4#Ghz\n",
    "\n",
    "\n",
    "from astropy.cosmology import WMAP9 as cosmo\n",
    "ld=[]\n",
    "for x in range(len(df)):\n",
    "    ld.append(cosmo.luminosity_distance(df['Z'][x]).value * 3.086e22) #convert from MPc to m.\n",
    "df['ld'] = ld\n",
    "\n",
    "#Calculate the luminosity at 1.4Ghz\n",
    "df['lum'] = 4*np.pi*(df['ld']**2)/((1+df['Z'])**(1+radio_index)*(EMU_Freq/Comp_Freq)**radio_index)*(df['Total_flux']*1e-26)\n",
    "Min_lum_line = 4*np.pi*(df['ld']**2)/((1+df['Z'])**(1+radio_index)*(EMU_Freq/Comp_Freq)**radio_index)*(flux_limit*1e-26)\n",
    "\n",
    "#2.4 Plot the results\n",
    "fig3, [ax1,ax2] = plt.subplots(1,2)\n",
    "fig3.suptitle('The distribution of readings across\\n the EMU GAMA Survey', fontsize=15)\n",
    "ax1.scatter(df['Z'],np.log10(df['lum']))\n",
    "ax1.set_xlabel('Redshift')\n",
    "ax1.set_ylabel('$Log_{10} L (Watts)$')\n",
    "ax1.scatter(df['Z'], np.log10(Min_lum_line), s=2, label = str(flux_limit*1000) + \"mJy\")\n",
    "ax1.legend(fontsize=10)\n",
    "ax2.scatter(df['Z'], np.log10(df['Total_flux']), c = 'r')\n",
    "ax2.set_xlabel('Redshift')\n",
    "ax2.set_ylabel('$Log_{10} S (Janksy)$')\n",
    "\n",
    "\n",
    "#%% STEP 6 Calculate Vmax using Novak Vmax method\n",
    "\n",
    "#6.1 Define our Redshift bins\n",
    "redshift_bins=[0.0,0.1,0.4,0.6,0.8,1.0,1.3,1.6,2.0]\n",
    "\n",
    "#7.2 Create our labels to slice up the data\n",
    "z_labels=['<' + str(redshift_bins[0])] #an array to create labels of the form <0.4, <0.6 etc, that will bin\n",
    "          # each data point starting at 0.0\n",
    "for ii in range(len(redshift_bins)-1):\n",
    "    z_labels.append('<' + str(redshift_bins[ii+1]))\n",
    "#Give each data point an according bin\n",
    "df['z_bin'] = pd.cut(np.array(df['Z']), bins=redshift_bins, labels=z_labels[1:]) #note, we need to remove the 0.0 from the labels for\n",
    "df['min_z'] = pd.cut(np.array(df['Z']), bins=redshift_bins, labels = redshift_bins[:-1])    \n",
    "\n",
    "\n",
    "print(df['z_bin'].value_counts(dropna=False))  #Check the distribution of the bins \n",
    "\n",
    "Vmax_method=\"Observable\"\n",
    "\n",
    "if Vmax_method == \"Novak\":\n",
    "    \n",
    "    \n",
    "    #6.1.1. Parameters for that dictate the correction\n",
    "    #2.6 degrees squared for the Cosmos field \n",
    "    survey_area = 82.7\n",
    "    survey_area = survey_area/41253 #portion of the sky surveyed.\n",
    "    \n",
    "    #Note the correction factor is not needeed\n",
    "    # correction = survey_area * df['radio_cc'] * df['op_cc'] #eq 5. Novak 2017.\n",
    "    # df['corr'] = correction\n",
    "    \n",
    "    #6.1.2 find which bin each redshift value is in\n",
    "    z = df['Z']\n",
    "    bin_num = []\n",
    "    \n",
    "    for value in z:\n",
    "        for ii in range(len(redshift_bins)):\n",
    "            if value<redshift_bins[ii]:\n",
    "                bin_num.append(ii-1)\n",
    "                break\n",
    "        else:\n",
    "            bin_num.append(-1)        \n",
    "    #every point now has an index for which bin it is in, if bin number -1, then galaxy is outside of redshift bins\n",
    "    \n",
    "    #6.2 Calculate the space contained within each redshift bin using the method outlined in Novak 2017. (Sliced integral approx)\n",
    "    Vbin=[]\n",
    "    stepsize=0.01\n",
    "    for n in range(len(redshift_bins)-1):\n",
    "        z_range = np.arange(redshift_bins[n],redshift_bins[n+1]+stepsize,stepsize)\n",
    "        #z_range = redshift_bins[n],redshift_bins[n+1]\n",
    "        Volume=[]\n",
    "        for i in range(len(z_range)-1): #Run through z_range, and sum up the Volume eq 8 Enia et. al '22\n",
    "            Volume1= (4*np.pi/3) * cosmo.comoving_distance(z_range[i])**3\n",
    "            Volume2= (4*np.pi/3) * cosmo.comoving_distance(z_range[i+1])**3\n",
    "            Volume.append((Volume2 - Volume1).value)\n",
    "        Vbin.append(sum(Volume))\n",
    "        \n",
    "    #Lets add a fail safe for things outside the redshift bins\n",
    "    Vbin.append(0)\n",
    "        \n",
    "    # 6.3 Knowing which bin each galaxy is in, and each Volume contained in each bin vMax can be calculated.\n",
    "    Vmax=[]\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        Vol = Vbin[bin_num[i]]\n",
    "        #Vmax.append(Vol* df['corr'][i])\n",
    "        Vmax.append(Vol*survey_area)\n",
    "    \n",
    "    # We calculate a Vmax value for each and can add it to the database\n",
    "    df['Vmax']=Vmax\n",
    "\n",
    "if Vmax_method == \"Observable\":\n",
    "    \n",
    "    \n",
    "    dmax = []\n",
    "    dmax = np.sqrt(df['lum']*((1+df['Z'])**(1+radio_index)) /((flux_limit*1e-26) * (4*np.pi ))) #This will be in metres\n",
    "    dmax=dmax * 3.24078e-23 #convert to Mpc\n",
    "    df['dmax']= dmax\n",
    "\n",
    "    # Calculate the seven z min distance each box is split up into\n",
    "    min_d = []\n",
    "    for value in redshift_bins[:-1]:\n",
    "        min_d.append(cosmo.comoving_distance(value).value)\n",
    "    \n",
    "    #Create an array to store the minimum and actual distances in Mpc using the minimum redshifts of the redshift bins and redshit values.\n",
    "    dmin = []\n",
    "    dactual=[]\n",
    "    for x in range(len(df)):\n",
    "        dmin.append(cosmo.comoving_distance(df['min_z'][x]).value) #Using Cosmo luminosity distance gives us the answer in Mpc.\n",
    "        dactual.append(cosmo.comoving_distance(df['Z'][x]).value)\n",
    "    df['dmin']=dmin\n",
    "    df['dactual']=dactual\n",
    "\n",
    "    #This is the distance we can use to calculate the Vmax for each object, using the area of sky surveyed.\n",
    "    #survey_area = (11**2)/(60**2) #convert from  sqaure of 11x11 arc minutes to square degree\n",
    "\n",
    "    #Tanias paper 82.7 EMU Data covering\n",
    "    #This should be 50.6 degrees squared for the G23 field \n",
    "    survey_area = 82.7\n",
    "    survey_area = survey_area/41253 #portion of the sky surveyed.\n",
    "    \n",
    "    # We can now calculate the Maximum Volume each galaxy could've been observed in.\n",
    "    Vmax = (4/3) * np.pi * (df['dmax'])**3 * survey_area\n",
    "    Vmin = (4/3) * np.pi * (df['dmin'])**3 * survey_area\n",
    "\n",
    "    # We calculate a Vmax value for each and can add it to the database\n",
    "    df['Vmax']=Vmax\n",
    "    df['Vmin']=Vmin\n",
    "    \n",
    "    #Subtract the Vmin from the Vmax so that the furtherest distance observable from the given bin\n",
    "    #can be calculated \n",
    "    df['Vmax']=df['Vmax']-df['Vmin']\n",
    "    \n",
    "    \n",
    "#%% Step 7 Classify the data\n",
    "\n",
    "\n",
    "#7.2 Create our labels to slice up the data\n",
    "z_labels=['<' + str(redshift_bins[0])] #an array to create labels of the form <0.4, <0.6 etc, that will bin\n",
    "          # each data point starting at 0.0\n",
    "for ii in range(len(redshift_bins)-1):\n",
    "    z_labels.append('<' + str(redshift_bins[ii+1]))\n",
    "#Give each data point an according bin\n",
    "df['z_bin'] = pd.cut(np.array(df['Z']), bins=redshift_bins, labels=z_labels[1:]) #note, we need to remove the 0.0 from the labels for\n",
    "df['min_z'] = pd.cut(np.array(df['Z']), bins=redshift_bins, labels = redshift_bins[:-1])    \n",
    "\n",
    "\n",
    "print(df['z_bin'].value_counts(dropna=False))  #Check the distribution of the bins \n",
    "\n",
    "\n",
    "#%% STEP 8 Calculate Phi\n",
    "#How many bins do you want for our lum funcs?\n",
    "n_bins=10\n",
    "#Do we want error bars\n",
    "error_bars='y'\n",
    "#Do we want Novaks published data to compare\n",
    "lit=\"Novak\"\n",
    "#Check the data\n",
    "print(df['z_bin'].value_counts(dropna=False))  #Check the distribution of the bins \n",
    "\n",
    "\n",
    "if lit == \"Enia\":\n",
    "    redshift_bins=[0.0,0.4,0.7,1.0,2.0,3.0,10.0] #From Enia 2022\n",
    "elif lit ==\"Novak\":\n",
    "    redshift_bins=[0.0,0.1,0.4,0.6,0.8,1.0,1.3,1.6,2.0] #Novak amended cut off with 2.0->10\n",
    "    \n",
    "#7.2 Create our labels to slice up the data\n",
    "z_labels=['<' + str(redshift_bins[0])] #an array to create labels of the form <0.4, <0.6 etc, that will bin\n",
    "              # each data point starting at 0.0\n",
    "for ii in range(len(redshift_bins)-1):\n",
    "    z_labels.append('<' + str(redshift_bins[ii+1]))\n",
    "#Give each data point an according bin\n",
    "df['z_bin'] = pd.cut(np.array(df['Z']), bins=redshift_bins, labels=z_labels[1:]) #note, we need to remove the 0.0 from the labels for\n",
    "df['min_z'] = pd.cut(np.array(df['Z']), bins=redshift_bins, labels = redshift_bins[:-1])    \n",
    "\n",
    "print(df['z_bin'].value_counts(dropna=False))  #Check the distribution of the bins \n",
    "\n",
    "\n",
    "#4.1 Create an array to store the Phi and Lum values that are to be plotted\n",
    "Phi_arr = []\n",
    "Lum_arr = []\n",
    "error_arr=[] \n",
    "counts_arr=[] #An array to store the counts for each readshift bings\n",
    "#Since we wish to do a log plot we will need to define the upper and lower limits seperately for an assymetircal graph\n",
    "upper_err_arr=[] #Array to store all the upper errors arrays for each redshift bin\n",
    "lower_err_arr=[] #Array to store all the lower errors arrays for each redshift bin\n",
    "\n",
    "#4.2 Loop through each redshift bin and calculate phi, the lumminosity data and error for phi for each redshift bin\n",
    "for x in range(len(z_labels[1:])):\n",
    "    label = z_labels[x+1]\n",
    "    zslice = df[df['z_bin']==label] #select our first redsfhit bin i.e. 0<z<label[1]\n",
    "    zslice =zslice[['lum','Total_flux','Vmax','Z']] #take the useful values, i.e. luminosity and Vmax values\n",
    "    #4.2.1 Not that we have removed all the outliers we can calculate the luminosity\n",
    "    lum = zslice['lum']#select the luminosity data for the redshift bin\n",
    "    V_list = zslice['Vmax'] #select the redshift data for the redshift bin\n",
    "    print('For redshift between ' + str(z_labels[x]) + ' and ' +str(label))\n",
    "    \n",
    "    #4.2.1. Find any outliers in the luminosity data using basic Standard Deviation, replace later\n",
    "    # with Grubbs test for outliers\n",
    "    mean = np.mean(lum)\n",
    "    std = np.std(lum)\n",
    "    #Find any outliers outside 3 standard deviations\n",
    "    outlier=[]\n",
    "    for i in lum:\n",
    "        if i > mean + 3*std or i < mean - 3*std:\n",
    "            outlier.append(i)\n",
    "    #remove outliers from the luminosity list\n",
    "    for value in outlier:\n",
    "        zslice.drop(zslice[zslice['lum'] == value].index, inplace = True)\n",
    "             \n",
    "    print(\"There were \" + str(len(outlier)) +\" outliers removed from data list \" +\n",
    "          str(len(lum)) +\" long\")\n",
    "    \n",
    "    \n",
    "    #4.2.1 Not that we have removed all the outliers Redefine the luminosity and V_list\n",
    "    lum = zslice['lum']#select the luminosity data for the redshift bin\n",
    "    V_list = zslice['Vmax'] #select the redshift data for the redshift bin\n",
    "\n",
    "    #4.2.2 Bin all the luminosities in the chosen redshift bin   \n",
    "    counts, bin_edges = np.histogram(np.log10(lum), bins=n_bins)\n",
    "    print('The number of galaxies in each bin is ' + str(counts)) #check to see how many galaxies fall in each luminosity bin for this slice\n",
    "     \n",
    "    #Lets order the zslice unsing the bin edges and counts\n",
    "    zslice = zslice.sort_values(by=['lum'])\n",
    "    # Calculate phi and the err for each bin\n",
    "    phi =[] #an array to store the phi values for the chosen luminosity bins\n",
    "    err=[] #an array to store the error values for the chosen luminosity bins\n",
    "    l_half=[] #an array to store the width of each luminosity bin\n",
    "    l_average=[]\n",
    "    zero_count=0 # a variable to account for any bins that don't have a luminosity reading\n",
    "\n",
    "    #4.2.3 Loop through each bin of the histogram and calculate phi and luminosity values\n",
    "    for n in range(n_bins):\n",
    "\n",
    "        if counts[n] > 0:\n",
    "            delta_l = bin_edges[n+1]-bin_edges[n] #remember to remove the log\n",
    "            #Create the luminosity reading\n",
    "            l_half.append((bin_edges[n+1]+bin_edges[n])/2)\n",
    "            l_average.append(np.log10(np.mean(zslice['lum'][0:counts[n]])))\n",
    "            #Create our phi value\n",
    "            phi.append(1/delta_l * sum((1/(zslice['Vmax']))[0:counts[n]]))\n",
    "            #Calculate the error bars\n",
    "            error=(1/delta_l * np.sqrt(sum((1/(zslice['Vmax']**2))[0:counts[n]])))\n",
    "            if counts[n] <=10: #check if small number poisson statistics\n",
    "                err.append(phi[n-zero_count]-phi[n-zero_count]*(ap.stats.poisson_conf_interval(counts[n]))[0]/counts[n]) #add the small number error to the err array\n",
    "            else:\n",
    "                err.append(error) #Novak 2017 eq (6)\n",
    "            #now to chop the number of points used from the zslice for the sake of the loop\n",
    "            zslice=zslice[counts[n]:]\n",
    "        else:\n",
    "            zero_count+=1\n",
    "    \n",
    "    counts_arr.append(counts)\n",
    "    Phi_arr.append(phi)\n",
    "    error_arr.append(err)\n",
    "    Lum_arr.append(l_average) \n",
    "    \n",
    "\n",
    "upper_err_arr=np.add(Phi_arr,error_arr)\n",
    "lower_err_arr=np.subtract(Phi_arr,error_arr)\n",
    "\n",
    "\n",
    "#NOTE SOME ERRORS MAY BE LARGER THAN THEIR PHI VALUES. i.e. if PHI is 1.6e-3, but the error is \n",
    "#1.8e-3, that simply is a issue with the PHI valye being so close to zero. \n",
    "#I must create a clause s.t. if error is larger the phi value, the error must equal the phi value.\n",
    "#OR if the lower err_arr is <0, set it to 0, or set the error to be exactly equal to the value.\n",
    "for jj in range(len(z_labels[1:])):\n",
    "    lower_err_arr[jj][lower_err_arr[jj]<0]=0\n",
    "\n",
    "#%%\n",
    "#4.3 Graph the values of Phi and Lum for all redshift bins\n",
    "counter = len(z_labels)-1 #Find how many redshift bins we wish to graph\n",
    "\n",
    "#4.3.1 Plot\n",
    "fig, ax = plt.subplots(2,int(counter/2), sharex=(True), sharey=True)\n",
    "fig.suptitle('Luminosity functions for various redshift \\n slices using the EMU-G23 data', fontsize=10)\n",
    "fig.text(0.5, 0.02, r'$logL_{1.4GHz}[W Hz^{-1}]$', ha='center', fontsize =10)\n",
    "fig.text(0.02, 0.5, r'$log\\Phi[Mpc^{-3}dex^{-1}]$', va='center', rotation='vertical', fontsize=10)\n",
    "plt.subplots_adjust(bottom=0.15, left=0.11, top=0.9, wspace=0, hspace=0)\n",
    "plt.yscale('log')\n",
    "   \n",
    "for ii in range(counter): #RUN THE LOOP THROUGH THE LABELLED/FILTERED INSTEAD OF SLICES\n",
    "    textstr=str(z_labels[ii][1:]) + '<z' +str(z_labels[ii+1])\n",
    "    print(ii)\n",
    "    \n",
    "    #Create out asymetricerror\n",
    "    asymetricerror=[lower_err_arr[ii],upper_err_arr[ii]]\n",
    "    \n",
    "    print(asymetricerror)\n",
    "    \n",
    "    # Plot the resulting phi and Luminosity on the two rows of the figure\n",
    "    if ii < int(counter/2):\n",
    "        ax[0,ii].scatter(Lum_arr[ii],Phi_arr[ii], label='EMU G23')\n",
    "        ax[0,ii].tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax[0,ii].tick_params(axis='both', which='minor', labelsize=10)\n",
    "        ax[0,ii].label_outer()\n",
    "        ax[0,ii].grid()\n",
    "        ax[0,ii].text(0.02, 0.1, textstr, transform=ax[0,ii].transAxes, fontsize=10, verticalalignment='bottom', color='red',\n",
    "                      bbox = dict(facecolor='white', edgecolor='red', pad=2.0))\n",
    "        if error_bars == \"y\":\n",
    "            ax[0,ii].errorbar(Lum_arr[ii],Phi_arr[ii], yerr=np.array(asymetricerror), ls='none')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #ax[1,int(ii-counter/2)].set_yscale('log')\n",
    "        ax[1,int(ii-counter/2)].scatter(Lum_arr[ii],Phi_arr[ii])\n",
    "        ax[1,int(ii-counter/2)].label_outer()\n",
    "        ax[1,int(ii-counter/2)].text(0.02, 0.9, textstr, transform=ax[1,int(ii-counter/2)].transAxes, fontsize=10, \n",
    "                    verticalalignment='bottom', color='red', bbox = dict(facecolor='white', edgecolor='red', pad=2.0))\n",
    "        ax[1,int(ii-counter/2)].grid()\n",
    "        ax[1,int(ii-counter/2)].tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax[1,int(ii-counter/2)].tick_params(axis='both', which='minor', labelsize=10)\n",
    "        if error_bars == \"y\":\n",
    "            ax[1,int(ii-counter/2)].errorbar(Lum_arr[ii],Phi_arr[ii], yerr=np.array(asymetricerror), fmt='o', ls='none')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    print('The Phi values for each bins are ', ['%.2f' % np.log10(elem) for elem in Phi_arr[ii]])\n",
    "    print(\"The number of points in each bin are \", counts_arr[ii])\n",
    "    print('The data is plotted at luminosities of ', ['%.2f' % elem for elem in Lum_arr[ii]])\n",
    "    print('\\n')\n",
    "    \n",
    "#Novak Findings Overlay\n",
    "if lit==\"Novak\":\n",
    "        Novak_L = [[21.77,22.15,22.46,22.77,23.09,23.34],[22.29,22.54,22.80,23.04,23.31,23.68],\n",
    "                   [22.61,22.84,23.11,23.40,23.71,24.06],[22.85,23.05,23.30,23.54,23.81,24.11],\n",
    "                   [23.10,23.31,23.57,23.84,24.06,24.38],[23.32,23.53,23.81,24.15,24.39,24.82],\n",
    "                   [23.55,23.72,23.98,24.28,24.536,24.90]] #Need to change these to Log Luminosities\n",
    "       \n",
    "        Novak_Phi = [[-2.85,-2.88,-3.12,-3.55,-4.05,-4.63],[-2.97,-3.19,-3.33,-3.67,-4.32,-5.05],\n",
    "                     [-2.89,-3.13,-3.47,-3.99,-4.68,-5.43],[-3.01,-3.13,-3.45,-3.85,-4.31,-4.89],\n",
    "                     [-3.19,-3.42,-3.86,-4.15,-4.74,-5.25],[-3.36,-3.55,-4.10,-4.53,-5.30,-5.94],\n",
    "                     [-3.47,-3.66,-4.15,-4.56,-5.06,-5.86]] \n",
    "        Novak_Phi = [np.power(10, x) for x in Novak_Phi]\n",
    "        \n",
    "        ax[0,1].scatter(Novak_L[0],Novak_Phi[0], label = \"Novak 2017\")\n",
    "        ax[0,2].scatter(Novak_L[1],Novak_Phi[1])\n",
    "        ax[0,3].scatter(Novak_L[2],Novak_Phi[2])\n",
    "        ax[1,0].scatter(Novak_L[3],Novak_Phi[3])\n",
    "        ax[1,1].scatter(Novak_L[4],Novak_Phi[4])\n",
    "        ax[1,2].scatter(Novak_L[5],Novak_Phi[5])\n",
    "        ax[1,3].scatter(Novak_L[6],Novak_Phi[6])\n",
    "        \n",
    "#Enia Findings Overlay\n",
    "if lit ==\"Enia\":  \n",
    "    Enia_L=[[21.48,21.99,22.42,22.84],[22.16,22.55,22.96,23.38],\n",
    "            [22.58,22.88,23.26,23.63],[23.04,23.57,23.96,24.35],\n",
    "            [23.59,23.94,24.27,24.60],[23.70,24.09,24.27,24.69]]\n",
    "    Enia_Phi=[[-2.40,-2.77,-3.20,-3.42],[-2.38,-2.75,-3.19,-3.80],\n",
    "              [-2.54,-2.84,-3.19,-3.99],[-2.48,-3.55,-4.16,-4.72],\n",
    "              [-2.94,-3.72,-4.26,-4.81],[-3.26,-3.98,-4.52,-4.98]]\n",
    "                    \n",
    "                        \n",
    "    ax[0,0].scatter(Enia_L[0],Enia_Phi[0])\n",
    "    ax[0,1].scatter(Enia_L[1],Enia_Phi[1], label = \"Enia 2022\")\n",
    "    ax[0,2].scatter(Enia_L[2],Enia_Phi[2])\n",
    "    ax[1,0].scatter(Enia_L[3],Enia_Phi[3])\n",
    "    ax[1,1].scatter(Enia_L[4],Enia_Phi[4])\n",
    "    ax[1,2].scatter(Enia_L[5],Enia_Phi[5])\n",
    "      \n",
    "        \n",
    "\n",
    "handles, labels = ax[0,1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,bbox_to_anchor = (1,1.03), loc='upper right', fontsize=9)\n",
    "\n",
    "\n",
    " #%%LETS TAKE THE FIRST 4 BOXES THAT ARE VALID TO USE\n",
    "\n",
    "Phi=Phi_arr[0:4]\n",
    "Lum = Lum_arr[0:4]\n",
    "Lower_err = lower_err_arr[0:4]\n",
    "Upper_err = upper_err_arr[0:4]\n",
    "\n",
    "\n",
    "\n",
    "#%%Lets look at these 4 areas\n",
    "\n",
    "# Create subplots with shared y-axis\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8), sharey=True, sharex=True)\n",
    "fig.suptitle('A closer look at the luminosity functions of EMU and Novak')\n",
    "\n",
    "# Flatten the axs array for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Iterate over the datasets and create plots\n",
    "for i, ax in enumerate(axs):\n",
    "    x = Lum[i]\n",
    "    y = Phi[i]\n",
    "    lower_err = Lower_err[i]\n",
    "    upper_err = Upper_err[i]  \n",
    "    # Plot the data points with asymmetric error bars\n",
    "    ax.errorbar(x, y, yerr=[lower_err,upper_err], fmt='o', capsize=3, label='EMU')\n",
    "    #ax.errorbar(x, y, yerr=[0.3*data for data in y], fmt='o', capsize=3) #custom error.   \n",
    "    # Set plot labels and title\n",
    "    ax.set_title(str(redshift_bins[i])+'<z<'+str(redshift_bins[i+1]))\n",
    "    ax.set_yscale('log')  \n",
    "    # Add gridlines\n",
    "    ax.grid(True)\n",
    " # Add secondary data to the final three plots\n",
    "    if i > 0:\n",
    "        ax.scatter(Novak_L[i-1],Novak_Phi[i-1], c='orange', label='Novak')\n",
    "        ax.legend()\n",
    "\n",
    "# Set a common y-axis label\n",
    "fig.text(0.04, 0.5, 'Phi', va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.04, 'Lum', va='center', rotation='horizontal')\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "#%% GO TO 4_PARAMETER FIT IN ULTRANEST\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%% Wish to fit a Saunders function, from Saunders et al 1990, Novak et al. 2017\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
