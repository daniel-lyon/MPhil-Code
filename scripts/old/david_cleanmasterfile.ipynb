{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct 22 13:57:10 2023\n",
    "\n",
    "@author: BigD\n",
    "\"\"\"\n",
    "\n",
    "#Import all moduels as required\n",
    "import astropy as ap\n",
    "from astropy.stats import poisson_conf_interval\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "from scipy.stats import (moyal, invgamma, lognorm, burr, beta, gamma, expon, exponpow, chi2, cauchy) \n",
    "from scipy import stats\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "\n",
    "\n",
    "# STEP 1 ORGANISE THE DATA\n",
    "\n",
    "# Import the crossmatched data \n",
    "data = Table.read('/Users/BigD/cloudstor/Data/EMU - GAMA - G23')\n",
    "data = data.to_pandas()\n",
    "\n",
    "#G23_data = Table.read('C:/Users/BigD/cloudstor/Data/GAMA/GAMA - Michael Brown/GAMA_G23_v01_mu3_20191202.fits')\n",
    "#G23_data = G23_data.to_pandas()\n",
    "\n",
    "#import the EMU data\n",
    "EMU_data = Table.read('/Users/BigD/cloudstor/Data/EMU/G23-ASKAP-EMUES-master-cat.fits', format='fits')\n",
    "EMU_data= EMU_data.to_pandas()\n",
    "\n",
    "#merge these two groups so that you have a ~40,000 sources table with 93 columns.\n",
    "data = pd.merge(data, EMU_data, how=\"outer\", on = 'Source_Name')\n",
    "data = data[['Source_Name','RA_y','E_RA_y','DEC_y','E_DEC_y' ,'Total_flux_y','E_Total_flux_y','Z','NQ','Separation_x']]\n",
    "data = data.rename(columns = {\"RA_y\":\"RA\", 'E_RA_y':'E_RA', 'DEC_y':'DEC', 'E_DEC_y':'E_DEC','Total_flux_y':'Total_flux','E_Total_flux_y':'E_Total_flux','Separation_x':'Separation' })\n",
    "\n",
    "#del(EMU_data, data)\n",
    "\n",
    "#We now have the crossmatched data set of EMU and G23, outlining:\n",
    "print(list(data))\n",
    "\n",
    "#create a column containing the rel error\n",
    "data['rel_err']=data['E_Total_flux']/data['Total_flux']\n",
    "\n",
    "#%% STEP 2 Visualise the data\n",
    "\n",
    "from observationsofdataset import Visualiseradiodata\n",
    "\n",
    "Visualiseradiodata(data)\n",
    "\n",
    "#we can see that there needs to be a lot of completness corrections\n",
    "    \n",
    "#%% STEP 3\n",
    "# Import required modules\n",
    "import matplotlib.pyplot as plt\n",
    "from completenesscorrection import getpdfs\n",
    "\n",
    "# STEP 3: CREATE THE PDFS FOR MISSING REDSHIFT AND REDSHIFT ERRORS\n",
    "# NOTE: THIS STEP WILL CREATE A DATAFRAME DF THAT IS POSITIVE FLUX AND Z>0\n",
    "\n",
    "# 3.1: Create pdfs for the redshift\n",
    "tol = 10  # The minimum number of data points in each bin to form a probability distribution function\n",
    "n = 10  # The number of magnitude bins the data will be separated into\n",
    "df, pdfs, labels, use_labels, bins = getpdfs(data, n, tol)\n",
    "\n",
    "# 3.2: Show what portion of the data set was used to create pdf's\n",
    "fig1, ax = plt.subplots(1, 1)\n",
    "for label in labels:\n",
    "    ax.hist(df[df['bin'] == label]['mag'], alpha = 1, rwidth = 1, \n",
    "            range = (df['mag'].min(), df['mag'].max()), bins = 200, label = label)\n",
    "ax.set_xlabel('$Mag_{AB}$')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.suptitle('The different magnitudes that created the pdfs')\n",
    "plt.legend(fontsize = 10)\n",
    "\n",
    "datapoint_count = sum(df['bin'].str.contains(label).sum() for label in use_labels)\n",
    "percentage = \"{0:.3%}\".format(datapoint_count / len(df))\n",
    "print(f\"pdf's were calculated from {percentage} of the datapoints that have a positive flux and greater than zero redshift\")\n",
    "\n",
    "\n",
    "#%% STEP 4 FILL IN THE MISSING DATA USING THE CREATED PDF'S\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fillinmissing import Fillinmissing\n",
    "\n",
    "# STEP 4: FILL IN THE MISSING DATA USING THE CREATED PDF'S\n",
    "\n",
    "# Calculate the magnitudes for each data point\n",
    "# Convert everything to AB magnitude\n",
    "data['mag'] = 25 - 2.5 * np.log10(data['Total_flux'])\n",
    "\n",
    "# Use the bins that the pdf's were created on, however add an upper and lower bound [ 0,50]\n",
    "# so to capture anything that falls outside the range covered in the spectro sample\n",
    "bins = [0] + bins + [50]  # Add lower and upper boundaries\n",
    "# Also create an upper bound for our labels\n",
    "labels.append(str(labels[-1].replace('<', \">\")))\n",
    "\n",
    "# Bin in accordance with the labels for which the pdf's were calculated\n",
    "data['bin'] = pd.cut(data['mag'], bins=bins, labels=labels)\n",
    "# Check the distribution of the bins\n",
    "print(data['bin'].value_counts(dropna=False))\n",
    "\n",
    "# Count what percentage of data the pdf's don't account for\n",
    "datapoint_count = sum(data['bin'].value_counts(dropna=False)[label] for label in use_labels)\n",
    "percentage = \"{0:.3%}\".format(datapoint_count / len(data))\n",
    "print(f\"The pdf's can be utilized for {percentage} of the EMU data\")\n",
    "\n",
    "# 4.1: Fill in the missing data and draw comparison of the mock and given data\n",
    "df = Fillinmissing(data, bins, labels, use_labels, pdfs)\n",
    "\n",
    "# Note: Losing about 590 readings with only a 98.475% coverage\n",
    "# For now, this is good enough. Let's continue with the 98.475% data for the luminosity functions.\n",
    "df = df[df['Z'] > 0]\n",
    "\n",
    "# Display a magnitude vs redshift graph for the filled-in data\n",
    "fig2, ax = plt.subplots(1, 1)\n",
    "ax.scatter(df['Z'], np.log10(df['Total_flux']))\n",
    "ax.set_xlabel('Redshift')\n",
    "ax.set_ylabel('Flux (Jy)')\n",
    "\n",
    "\n",
    "#%%\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.cosmology import WMAP9 as cosmo\n",
    "\n",
    "# STEP 5: Calculate the luminosity's for everything\n",
    "\n",
    "# 5.1: Find the turnaround of the flux\n",
    "fig1, ax1 = plt.subplots()\n",
    "counts, bin_edges = np.histogram(np.log10(df['Total_flux']), bins=200)\n",
    "ax1.hist(np.log10(df['Total_flux']), bins=200)\n",
    "ax1.set_title('The total flux as measured by EMU')\n",
    "ax1.set_xlabel('$Log_{10} S (Jansky)$')\n",
    "ax1.set_ylabel('counts') \n",
    "\n",
    "flux_limit_idx = np.argmax(counts)\n",
    "flux_limit = 10**bin_edges[flux_limit_idx]\n",
    "print(f'The flux limit is {flux_limit:.5f}')\n",
    "\n",
    "# 5.2: Reduce the data set to those values with only a positive redshift\n",
    "df = df[df['Z'] > 0].reset_index(drop=True)\n",
    "\n",
    "fig2, ax1 = plt.subplots()\n",
    "ax1.scatter(df['Z'], np.log10(df['Total_flux']))\n",
    "ax1.set_title('The distribution of flux readings across the EMU GAMA Survey')\n",
    "ax1.set_xlabel('Redshift')\n",
    "ax1.set_ylabel('$Log_{10} S (Janksy)$')\n",
    "\n",
    "# 5.4: Calculate the luminosity for everything\n",
    "flux_limit = 200e-6  # According to GÃ¼rkan the flux limits is 0.2mJy or 200uJy\n",
    "radio_index = -0.7\n",
    "EMU_Freq = 0.8875  # Ghz (997 Mhz)\n",
    "Comp_Freq = 1.4  # Ghz\n",
    "\n",
    "# Calculate luminosity distance and add it to dataframe\n",
    "df['ld'] = [cosmo.luminosity_distance(z).value * 3.086e22 for z in df['Z']]\n",
    "\n",
    "# Calculate the luminosity at 1.4Ghz\n",
    "lum_factor = 4*np.pi*(df['ld']**2)/((1+df['Z'])**(1+radio_index)*(EMU_Freq/Comp_Freq)**radio_index)\n",
    "df['lum'] = lum_factor * (df['Total_flux']*1e-26)\n",
    "\n",
    "Min_lum_line = lum_factor * (flux_limit*1e-26)\n",
    "\n",
    "# 5.4: Plot the results\n",
    "fig3, [ax1, ax2] = plt.subplots(1, 2)\n",
    "fig3.suptitle('The distribution of readings across the EMU GAMA Survey', fontsize=15)\n",
    "\n",
    "ax1.scatter(df['Z'], np.log10(df['lum']))\n",
    "ax1.scatter(df['Z'], np.log10(Min_lum_line), s=2, label=f'{flux_limit*1000}mJy')\n",
    "ax1.set_xlabel('Redshift')\n",
    "ax1.set_ylabel('$Log_{10} L (Watts)$')\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "ax2.scatter(df['Z'], np.log10(df['Total_flux']), c='r')\n",
    "ax2.set_xlabel('Redshift')\n",
    "ax2.set_ylabel('$Log_{10} S (Jansky)$')\n",
    "\n",
    "#%% STEP 6 Calculate Vmax using Novak Vmax method\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.cosmology import WMAP9 as cosmo\n",
    "\n",
    "# Define redshift bins and labels\n",
    "redshift_bins = [0.0, 0.1, 0.4, 0.6, 0.8, 1.0, 1.3, 1.6, 2.0]\n",
    "z_labels = ['<' + str(bin_edge) for bin_edge in redshift_bins]\n",
    "\n",
    "# Assign each data point to a redshift bin\n",
    "df['z_bin'] = pd.cut(df['Z'], bins=redshift_bins, labels=z_labels[1:])\n",
    "df['min_z'] = pd.cut(df['Z'], bins=redshift_bins, labels=redshift_bins[:-1])\n",
    "\n",
    "# Print the distribution of the bins\n",
    "print(df['z_bin'].value_counts(dropna=False))\n",
    "\n",
    "# Choose Vmax method\n",
    "Vmax_method = \"Observable\"\n",
    "\n",
    "if Vmax_method == \"Novak\":\n",
    "    survey_area = 82.7 / 41253  # Portion of the sky surveyed (82.7 deg^2).\n",
    "    z = df['Z']\n",
    "    bin_num = [next((i for i, bin_edge in enumerate(redshift_bins) if value < bin_edge), -1) for value in z]\n",
    "    \n",
    "    # Calculate the volume for each redshift bin\n",
    "    Vbin = [(4*np.pi/3) * (cosmo.comoving_distance(z2).value**3 - cosmo.comoving_distance(z1).value**3)\n",
    "            for z1, z2 in zip(redshift_bins[:-1], redshift_bins[1:])]\n",
    "    Vbin.append(0)  # Add zero volume for things outside the redshift bins\n",
    "    \n",
    "    # Calculate Vmax for each galaxy\n",
    "    df['Vmax'] = [Vbin[i] * survey_area for i in bin_num]\n",
    "\n",
    "elif Vmax_method == \"Observable\":\n",
    "    dmax = np.sqrt(df['lum'] * ((1 + df['Z']) ** (1 + radio_index)) / ((flux_limit * 1e-26) * (4 * np.pi)))\n",
    "    dmax *= 3.24078e-23  # Convert to Mpc\n",
    "    df['dmax'] = dmax\n",
    "    min_d = [cosmo.comoving_distance(bin_edge).value for bin_edge in redshift_bins[:-1]]\n",
    "    df['dmin'] = [cosmo.comoving_distance(min_z).value for min_z in df['min_z']]\n",
    "    df['dactual'] = [cosmo.comoving_distance(z).value for z in df['Z']]\n",
    "    \n",
    "    survey_area = 82.7 / 41253  # Portion of the sky surveyed (82.7 deg^2).\n",
    "    df['Vmax'] = (4 / 3) * np.pi * (df['dmax'] ** 3 - df['dmin'] ** 3) * survey_area\n",
    "    \n",
    "    \n",
    "    \n",
    "#%%\n",
    "# STEP 7: Classify the data\n",
    "\n",
    "# 7.1 Create our labels to slice up the data\n",
    "# Generate labels of the form <0.4, <0.6 etc, that will bin each data point starting at 0.0\n",
    "z_labels = ['<' + str(bin_edge) for bin_edge in redshift_bins]\n",
    "\n",
    "# Assign each data point to a redshift bin\n",
    "df['z_bin'] = pd.cut(df['Z'], bins=redshift_bins, labels=z_labels[1:])\n",
    "df['min_z'] = pd.cut(df['Z'], bins=redshift_bins, labels=redshift_bins[:-1])\n",
    "\n",
    "# Check the distribution of the bins\n",
    "print(df['z_bin'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "#%%\n",
    "#Step 8 Calculate PHI\n",
    "import pandas as pd\n",
    "import astropy as ap\n",
    "from astropy.stats import poisson_conf_interval\n",
    "\n",
    "# Define your settings\n",
    "n_bins = 10\n",
    "lit = \"Novak\"\n",
    "\n",
    "# Define redshift bins based on literature\n",
    "if lit == \"Enia\":\n",
    "    redshift_bins = [0.0, 0.4, 0.7, 1.0, 2.0, 3.0, 10.0]\n",
    "elif lit == \"Novak\":\n",
    "    redshift_bins = [0.0, 0.1, 0.4, 0.6, 0.8, 1.0, 1.3, 1.6, 2.0]\n",
    "\n",
    "# Create labels for the bins\n",
    "z_labels = list(zip(redshift_bins[:-1], redshift_bins[1:]))\n",
    "z_labels = ['<' + str(bin_edge) for bin_edge in redshift_bins]\n",
    "\n",
    "# Assign each data point to a bin\n",
    "df['z_bin'] = pd.cut(df['Z'], bins=redshift_bins, labels=z_labels[1:])\n",
    "df['min_z'] = pd.cut(df['Z'], bins=redshift_bins, labels=redshift_bins[:-1])\n",
    "\n",
    "# Initialize arrays to store results\n",
    "Phi_arr, Lum_arr, error_arr, counts_arr, upper_err_arr, lower_err_arr = ([] for _ in range(6))\n",
    "\n",
    "# Loop over each redshift bin\n",
    "for label in z_labels:\n",
    "    zslice = df[df['z_bin'] == label][['lum', 'Total_flux', 'Vmax', 'Z']]\n",
    "    \n",
    "    # Identify and remove outliers\n",
    "    mean, std = np.mean(zslice['lum']), np.std(zslice['lum'])\n",
    "    outliers = zslice[(zslice['lum'] > mean + 3*std) | (zslice['lum'] < mean - 3*std)]\n",
    "    zslice = zslice.drop(outliers.index)\n",
    "    print(f\"For redshift between {label[0]} and {label[1]}, {len(outliers)} outliers were removed from a list of {len(zslice)+len(outliers)}.\")\n",
    "\n",
    "    # Bin luminosities\n",
    "    counts, bin_edges = np.histogram(np.log10(zslice['lum']), bins=n_bins)\n",
    "    print(f\"The number of galaxies in each bin is {counts}. \\n\")\n",
    "\n",
    "    # Sort zslice by luminosity\n",
    "    zslice = zslice.sort_values(by='lum')\n",
    "\n",
    "    # Initialize arrays for this bin\n",
    "    phi, err, l_half, l_average = ([] for _ in range(4))\n",
    "    \n",
    "    # Loop over each luminosity bin\n",
    "    for n, count in enumerate(counts):\n",
    "        if count > 0:\n",
    "            # Perform calculations\n",
    "            delta_l = bin_edges[n+1] - bin_edges[n]\n",
    "            l_half.append((bin_edges[n+1] + bin_edges[n]) / 2)\n",
    "            l_average.append(np.log10(np.mean(zslice['lum'][:count])))\n",
    "            phi_val = 1 / delta_l * sum(1 / zslice['Vmax'][:count])\n",
    "            phi.append(phi_val)\n",
    "            error = 1 / delta_l * np.sqrt(sum(1 / zslice['Vmax'][:count]**2))\n",
    "            err.append(error if count > 10 else phi_val * (1 - ap.stats.poisson_conf_interval(count)[0] / count))\n",
    "            zslice = zslice[count:]\n",
    "\n",
    "    # Store results\n",
    "    counts_arr.append(counts)\n",
    "    Phi_arr.append(phi)\n",
    "    error_arr.append(err)\n",
    "    Lum_arr.append(l_average)\n",
    "    upper_err_arr.append(np.add(phi, err))\n",
    "    lower_err_arr.append(np.maximum(0, np.subtract(phi, err)))\n",
    "\n",
    "print(\"Analysis complete.\")\n",
    "\n",
    "#%%\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#4.3 Graph the values of Phi and Lum for all redshift bins\n",
    "counter = len(z_labels)-1 #Find how many redshift bins we wish to graph\n",
    "\n",
    "#4.3.1 Plot\n",
    "fig, ax = plt.subplots(2,int(counter/2), sharex=True, sharey=True)\n",
    "fig.suptitle('Luminosity functions for various redshift \\n slices using the EMU-G23 data', fontsize=10)\n",
    "fig.text(0.5, 0.02, r'$logL_{1.4GHz}[W Hz^{-1}]$', ha='center', fontsize=10)\n",
    "fig.text(0.02, 0.5, r'$log\\Phi[Mpc^{-3}dex^{-1}]$', va='center', rotation='vertical', fontsize=10)\n",
    "plt.subplots_adjust(bottom=0.15, left=0.11, top=0.9, wspace=0, hspace=0)\n",
    "plt.yscale('log')\n",
    "\n",
    "for ii in range(counter): #RUN THE LOOP THROUGH THE LABELLED/FILTERED INSTEAD OF SLICES\n",
    "    textstr = str(z_labels[ii][1:]) + '<z' + str(z_labels[ii+1])\n",
    "    print(ii)\n",
    "\n",
    "    # Create out asymetricerror\n",
    "    asymetricerror = np.array([lower_err_arr[ii], upper_err_arr[ii]])\n",
    "\n",
    "    # Determine row and column index for plotting\n",
    "    row = ii // int(counter/2)\n",
    "    col = ii % int(counter/2)\n",
    "    \n",
    "    # Plot the resulting phi and Luminosity on the two rows of the figure\n",
    "    ax[row, col].scatter(Lum_arr[ii], Phi_arr[ii], label='EMU G23')\n",
    "    ax[row, col].tick_params(axis='both', labelsize=10)\n",
    "    ax[row, col].label_outer()\n",
    "    ax[row, col].grid()\n",
    "    ax[row, col].text(0.02, 0.1, textstr, transform=ax[row, col].transAxes, fontsize=10,\n",
    "                      verticalalignment='bottom', color='red',\n",
    "                      bbox=dict(facecolor='white', edgecolor='red', pad=2.0))\n",
    "    \n",
    "    if error_bars == \"y\":\n",
    "        ax[row, col].errorbar(Lum_arr[ii], Phi_arr[ii], yerr=asymetricerror, ls='none')\n",
    "\n",
    "    print('The Phi values for each bins are ', ['%.2f' % np.log10(elem) for elem in Phi_arr[ii]])\n",
    "    print(\"The number of points in each bin are \", counts_arr[ii])\n",
    "    print('The data is plotted at luminosities of ', ['%.2f' % elem for elem in Lum_arr[ii]])\n",
    "    print('\\n')\n",
    "\n",
    "# Add additional overlays here, such as Novak and Enia findings\n",
    "\n",
    "handles, labels = ax[0, 1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(1, 1.03), loc='upper right', fontsize=9)\n",
    "    \n",
    "    \n",
    "#Novak Findings Overlay\n",
    "if lit==\"Novak\":\n",
    "        Novak_L = [[21.77,22.15,22.46,22.77,23.09,23.34],[22.29,22.54,22.80,23.04,23.31,23.68],\n",
    "                   [22.61,22.84,23.11,23.40,23.71,24.06],[22.85,23.05,23.30,23.54,23.81,24.11],\n",
    "                   [23.10,23.31,23.57,23.84,24.06,24.38],[23.32,23.53,23.81,24.15,24.39,24.82],\n",
    "                   [23.55,23.72,23.98,24.28,24.536,24.90]] #Need to change these to Log Luminosities\n",
    "       \n",
    "        Novak_Phi = [[-2.85,-2.88,-3.12,-3.55,-4.05,-4.63],[-2.97,-3.19,-3.33,-3.67,-4.32,-5.05],\n",
    "                     [-2.89,-3.13,-3.47,-3.99,-4.68,-5.43],[-3.01,-3.13,-3.45,-3.85,-4.31,-4.89],\n",
    "                     [-3.19,-3.42,-3.86,-4.15,-4.74,-5.25],[-3.36,-3.55,-4.10,-4.53,-5.30,-5.94],\n",
    "                     [-3.47,-3.66,-4.15,-4.56,-5.06,-5.86]] \n",
    "        Novak_Phi = [np.power(10, x) for x in Novak_Phi]\n",
    "        \n",
    "        for i, (L, Phi) in enumerate(zip(Novak_L, Novak_Phi)):\n",
    "            row = 0 if i < 4 else 1\n",
    "            col = i if row == 0 else i - 4\n",
    "            ax[row, col].scatter(L, Phi, label = \"Novak 2017\" if i==0 else None)\n",
    "        \n",
    "#Enia Findings Overlay\n",
    "if lit ==\"Enia\":  \n",
    "    Enia_L=[[21.48,21.99,22.42,22.84],[22.16,22.55,22.96,23.38],\n",
    "            [22.58,22.88,23.26,23.63],[23.04,23.57,23.96,24.35],\n",
    "            [23.59,23.94,24.27,24.60],[23.70,24.09,24.27,24.69]]\n",
    "    Enia_Phi=[[-2.40,-2.77,-3.20,-3.42],[-2.38,-2.75,-3.19,-3.80],\n",
    "              [-2.54,-2.84,-3.19,-3.99],[-2.48,-3.55,-4.16,-4.72],\n",
    "              [-2.94,-3.72,-4.26,-4.81],[-3.26,-3.98,-4.52,-4.98]]\n",
    "                    \n",
    "                        \n",
    "    for i, (L, Phi) in enumerate(zip(Enia_L, Enia_Phi)):\n",
    "        row = 0 if i < 3 else 1\n",
    "        col = i if row == 0 else i - 3\n",
    "        ax[row, col].scatter(L, Phi, label = \"Enia 2022\" if i==0 else None)\n",
    "    \n",
    "# Add legend\n",
    "handles, labels = ax[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor = (1,1.03), loc='upper right', fontsize=9)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
